! Simulating an Artificial Intelligence (AI) system in Fortran is an interesting challenge. 
! While Fortran isn't traditionally used for AI, its strength in numerical computations makes it suitable for implementing core components of AI, such as neural networks or optimization algorithms.

! Below is an example of a simple Feedforward Neural Network implemented in Fortran. 
! The network includes one hidden layer and uses sigmoid activation. It can be trained using stochastic gradient descent (SGD) for tasks like binary classification.

program simple_nn
  implicit none
  integer, parameter :: dp = selected_real_kind(15, 307)
  integer, parameter :: input_nodes = 2, hidden_nodes = 3, output_nodes = 1
  integer, parameter :: num_samples = 4, epochs = 10000
  real(dp), parameter :: learning_rate = 0.1

  ! Neural network weights and biases
  real(dp), dimension(hidden_nodes, input_nodes) :: weights_input_hidden
  real(dp), dimension(output_nodes, hidden_nodes) :: weights_hidden_output
  real(dp), dimension(hidden_nodes) :: bias_hidden
  real(dp), dimension(output_nodes) :: bias_output

  ! Data (XOR problem)
  real(dp), dimension(input_nodes, num_samples) :: inputs
  real(dp), dimension(output_nodes, num_samples) :: targets

  ! Initialize arrays
  call random_seed()
  call initialize_network(weights_input_hidden, weights_hidden_output, bias_hidden, bias_output)
  call generate_xor_data(inputs, targets)

  ! Training loop
  call train_network(inputs, targets, weights_input_hidden, weights_hidden_output, &
                     bias_hidden, bias_output, epochs, learning_rate)

  ! Testing
  call test_network(inputs, weights_input_hidden, weights_hidden_output, &
                    bias_hidden, bias_output)

contains

  subroutine initialize_network(wih, who, bh, bo)
    real(dp), dimension(:, :), intent(out) :: wih, who
    real(dp), dimension(:), intent(out) :: bh, bo
    integer :: i, j

    ! Initialize weights and biases with random values
    do i = 1, size(wih, 1)
       do j = 1, size(wih, 2)
          call random_number(wih(i, j))
          wih(i, j) = wih(i, j) - 0.5_dp
       end do
    end do
    do i = 1, size(who, 1)
       do j = 1, size(who, 2)
          call random_number(who(i, j))
          who(i, j) = who(i, j) - 0.5_dp
       end do
    end do
    call random_number(bh)
    bh = bh - 0.5_dp
    call random_number(bo)
    bo = bo - 0.5_dp
  end subroutine initialize_network

  subroutine generate_xor_data(inputs, targets)
    real(dp), dimension(:, :), intent(out) :: inputs
    real(dp), dimension(:, :), intent(out) :: targets

    ! XOR data
    inputs(:, 1) = [0.0_dp, 0.0_dp]
    inputs(:, 2) = [0.0_dp, 1.0_dp]
    inputs(:, 3) = [1.0_dp, 0.0_dp]
    inputs(:, 4) = [1.0_dp, 1.0_dp]
    targets(:, 1) = [0.0_dp]
    targets(:, 2) = [1.0_dp]
    targets(:, 3) = [1.0_dp]
    targets(:, 4) = [0.0_dp]
  end subroutine generate_xor_data

  subroutine train_network(inputs, targets, wih, who, bh, bo, epochs, lr)
    real(dp), dimension(:, :), intent(in) :: inputs, targets
    real(dp), dimension(:, :), intent(inout) :: wih, who
    real(dp), dimension(:), intent(inout) :: bh, bo
    integer, intent(in) :: epochs
    real(dp), intent(in) :: lr
    integer :: epoch, i
    real(dp), dimension(hidden_nodes) :: hidden_input, hidden_output
    real(dp), dimension(output_nodes) :: final_input, final_output
    real(dp), dimension(hidden_nodes) :: hidden_error
    real(dp), dimension(output_nodes) :: output_error

    do epoch = 1, epochs
       do i = 1, num_samples
          ! Forward pass
          hidden_input = matmul(wih, inputs(:, i)) + bh
          hidden_output = sigmoid(hidden_input)
          final_input = matmul(who, hidden_output) + bo
          final_output = sigmoid(final_input)

          ! Backward pass (Error computation)
          output_error = targets(:, i) - final_output
          hidden_error = matmul(transpose(who), output_error)

          ! Gradient descent
          who = who + lr * matmul(reshape(output_error * final_output * &
                       (1.0_dp - final_output), [output_nodes, 1]), &
                       reshape(hidden_output, [1, hidden_nodes]))
          bo = bo + lr * output_error * final_output * (1.0_dp - final_output)

          wih = wih + lr * matmul(reshape(hidden_error * hidden_output * &
                       (1.0_dp - hidden_output), [hidden_nodes, 1]), &
                       reshape(inputs(:, i), [1, input_nodes]))
          bh = bh + lr * hidden_error * hidden_output * (1.0_dp - hidden_output)
       end do
    end do
  end subroutine train_network

  subroutine test_network(inputs, wih, who, bh, bo)
    real(dp), dimension(:, :), intent(in) :: inputs
    real(dp), dimension(:, :), intent(in) :: wih, who
    real(dp), dimension(:), intent(in) :: bh, bo
    integer :: i
    real(dp), dimension(hidden_nodes) :: hidden_input, hidden_output
    real(dp), dimension(output_nodes) :: final_input, final_output

    print *, "Testing Neural Network:"
    do i = 1, num_samples
       hidden_input = matmul(wih, inputs(:, i)) + bh
       hidden_output = sigmoid(hidden_input)
       final_input = matmul(who, hidden_output) + bo
       final_output = sigmoid(final_input)
       print *, "Input:", inputs(:, i), "Predicted:", final_output
    end do
  end subroutine test_network

  function sigmoid(x) result(y)
    real(dp), intent(in) :: x
    real(dp) :: y
    y = 1.0_dp / (1.0_dp + exp(-x))
  end function sigmoid

end program simple_nn
